\documentclass[11pt]{article}

\usepackage[letterpaper]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{url}
\newcommand{\reals}{{\mathbb R}}

\title{Interim report\\CS6350 Machine Learning}
\author{Montgomery Carter \and Shaobo He}


\begin{document}
\maketitle

\section{Recap}
The goal of our proposed project is to predict the cuisine type of a
recipe based on the recipe's ingredient list\cite{kaggle-link}.  In
the dataset we are given a list of recipes.  For each recipe we are
given cuisine type and a list of ingredients.

\section{Milestones Achieved}
\label{sec:background}
%\begin{enumerate}
Our first task was to acquire the training and test
data\cite{download-link} from kaggle.com and ensure its formatting
was proper for machine processing.

Next we wrote a python program to read the data stored in JSON format
and convert it into a vector data structure so that it can be
processed smoothly by machine learning algorithms. To be more 
specific, each recipe becomes a vector where each dimension represents
a possible ingredient.  If the dimension's value is 0, the ingredient
is not present in the recipe; if the value is 1, the ingredient is
present in the recipe.  The final dimension of the vector is the
label.  Because we are dealing with multiple cuisines, we have a
multi-class classification problem (which is addressed in further
detail below).  For now, we mapped each possible cuisine to an integer
value, and this value is inserted as the last dimension in each recipe
vector.  We recognize that when we get further into analysis using
specific algorithms, it is likely that we will have to change the
representation of the label. 

Having transformed the data, we also did some preliminary analysis of
the data.  For example, we noticed that there are about 6000 different
possible ingredients (meaning 6000 dimensions per vector), which may
prove to be a challenge for linear classification.  However, we also
noticed that there are a number of different ingredients which seem to
be only slight spelling variations of the same actual ingredient.
Condensing such variants into a single ingredient may help with
reducing dimensionality.

We realize that perhaps our biggest obstacle with this project is
going to be the multi-class classification.  We haven't really
addressed such algorithms in class, so we are reading about ways to
address multi-class classification.\cite{wiki}
%\end{enumerate}


\section{Plan}
\label{sec:plan}

Our data is in relatively good shape to begin processing straight
away.  The exceptions to this, as mentioned above, are that the
dimensionality is relatively high, which may be problematic, and the
observation that there may essentially be duplicate ingredients which
can be joined together.

Initially, our plan is to attempt several machine learning algorithms
on the vectorized data without modifying the dimensionality.  This
will require us to come up with a strategy for doing multi-class
classification (one vs. one, or one vs. rest, or possibly other
approaches).

If any of these algorithms work well on the vectorized data as is,
then we're in good shape.  However, if we aren't seeing decent
results, we may look to reduce the dimensionality by joining
ingredients with spelling variations into a single dimension.  I am
hopeful that things will work well as is, because there are many, many
ingredients which are used in only one recipe.  This means that the
number of ingredients we're likely to see in test data should be far
less than the 6000 from the training data.

With that said, I just realized we'll need to handle the case where
unknown ingredients are encountered during testing.  


The next step is to apply machine learning algorithms on the training set. The first algorithm we would like to try is decision tree since it is a simple algorithm to learn data with multiple labels. Another reason is that we noticed that many ingredients are only associated with one individual cuisine. Thus it may be easy for decision tree to generalize the training data.

If decision tree does not work well, then we will turn to algorithms that convert the problem of multi-class classification into multiple binary-class classification problems. For instance, we can try one-vs-one multi-class classification\cite{wiki} and one-vs-res multi-class classification\cite{wiki}.  
%\begin{enumerate}
%\item Need to decide on algorithm
%\item Try one-vs-one multi-class classification
%\item Try one-vs-res multi-class classification
%\item Try other possible ways of doing multi-class classification (if any) 
%\end{enumerate}

%\newpage
\bibliography{refs}
\bibliographystyle{plain}
\end{document}
