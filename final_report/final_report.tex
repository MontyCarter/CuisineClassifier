\documentclass[11pt]{article}

\usepackage[letterpaper]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{caption}
\newcommand{\reals}{{\mathbb R}}

\title{Project Report\\CS6350 Machine Learning}
\author{Montgomery Carter \and Shaobo He}


\begin{document}
\maketitle

\section{Introduction}
The ability to predict a cuisine type of a recipe based on its
ingredients has some interesting real-world implications.  A retailer
can better target advertisements to its customers based on their food
preferences and buying habits.  For example, if a shopping cart
contains ingredients indicative of a certain cuisine, the retailer can
suggest other products that might be appealing to the consumer.

This is the problem that we address in our project.  The idea
originated from a competition on kaggle.com\cite{kaggle-link}. The
competition on  kaggle.com provides a list of recipes.  For each
recipe, the ingredients are given, as well as a cuisine type which
classifies each recipe.

We found this problem interesting in three ways. First, as mentioned,
the problem has real-world applications and might be interesting to
retailers, social media sites, and other organizations that aggregate
data about users and consumers.  Second, data given to us in this
project comes from real world, which provides us with an opportunity
to learn tricks to deal with unshaped data such as high
dimensionality, noise, and unseen labels.  Finally, we found this 
project to be interesting because it is a multi-class classification
problem with a relatively high number of classes.  Because we didn't
get much time to address multi-class classification in class, it has
been nice to get some experience with such problems.

Because this was our first real-world application of machine learning,
we didn't have much experience to guide our selection of a machine
learning algorithm.  As a result, our project became an exploration of
various machine learning algorithms.  We applied multiple machine
learning algorithms that we learned from class by leveraging
scikit-learn python package. Though we lacked real-world application
experience, we used theory learned in class to help guide our search
through the dozens of algorithms available in the scikit-learn
package.

In this paper we describe our investigation into the classification of
the cuisine of a recipe based on its ingredients.

\section{Problem Description and Preliminary Analysis}
As mentioned, we focus on classifying the cuisine of a recipe given
its ingredients.  We found pre-compiled recipe data on kaggle.com.
This dataset includes training data in a json file format.  The
training data is a list of recipes.  For each recipe, a list of
ingredients and a cuisine are given.  

A preliminary analysis of the data reveals some basic statistics about
the dataset.  The original training data includes 39,744 recipes taken
from 20 different cuisines, which use a total of 6,714 different
ingredients.  Of these 39,744 recipes, 7,838 are Italian, 6,438 are
Mexican, 4,320 are Southern US, 3,003 are Indian, 2,673 are Chinese,
and 2,646 are French.  Each of the remaining 14 cuisines contribute
fewer than 1,500 recipes each.

For a majority of cuisines, the most commonly used ingredients are
common across all cuisines.  For example, the most commonly used
ingredient of all cuisines is salt or a salt replacement like soy
sauce or fish sauce.  Although several of the top ingredients are
common across all cuisines, there are ingredients that we refer to as
indicator ingredients.  Such ingredients are only seen in a small
number of cuisines.  For example, garam marsala is \emph{only} used in
Indian food, where it is found in approximately 1/3 of all Indian
recipes.  Another example is fish sauce, which is only found in Asian
cuisines (Chinese, Korean, Filipino, etc.) Such ingredients
undoubtedly assist the prediction process as they reduce 
the number of cuisines possible for a recipe using such ingredients.  

Another observation to be noted for our problem is that a single set of
ingredients can map to multiple cuisines.  For example, French
baguettes and Italian breadsticks may call for the same ingredients in
different quantities.  This necessarily places a limit on the accuracy
we can hope to obtain in classifying recipes based on their
ingredients alone.


\section{Methodology}
We chose to implement our project Python due to its ability to rapidly
prototype solutions, our familiarity with the language, and the
availability of scikit-learn\cite{scikit}, an easy to use, well supported machine
learning package.  

We first perform preprocessing on the data.  Because there was no
useful test set provided on kaggle.com, we partition 90\% the
provided training data as our ``training set'', and 10\% as our ``test
set''. This provides us with the ability to measure the performance of
our final solution.  Two sets are then generated -- one containing the
ingredients and one containing the cuisine labels in use across the
training set.  These lists are then sorted.  The index of each
ingredient in the sorted set of ingredients becomes the feature 
dimension number for that ingredient, and the index of each cuisine in
the sorted set of cuisines becomes the label number for each cuisine
class.  A vector is then created for each recipe in the training and
test sets.  For each vector, the features get set to 1.0 if the
ingredient is present in the recipe, and the cuisine number becomes
the last element in the vector.  The vectors are then converted into
compressed row storage (CSR) format (this turned out to significantly
improve the performance of both reading and preprocessing the data, as 
well as the performance of the fitting and prediction).

As this is our first application of machine learning to a real-world
problem, we didn't have much experience to guide our selection of an
algorithm.  We began our investigation with a support vector machine
classifier.  After experimenting with several different SVM
configurations, we then proceeded to try two other classes of
learners: Naive Bayes and Decision Trees.  Initial results from these
categories of learners were encouraging.  However, seeing these
initial results, it became clear that we would likely see improvement
by using some ensemble methods.  Each of the algorithms used
automatically uses the concept of epochs (except K-Nearest Neighbors),
and we used the default stopping criteria to determine when to
terminate learning.  

For each of the algorithms used, 10-fold cross validation was used on
the training set.  For all algorithms taking hyper-parameters, a wide
array of values was tried for each hyper-parameter.  The
hyper-parameter combination yielding the best accuracy for each
algorithm tried is provided in the results section.  Our cross
validation framework utilizes a thread pool, allowing us to perform
cross-validation in a highly parallelized fashion.

After performing cross-validation for each algorithm, the best
hyper-parameter combination was then used to retrain the classifier
using the full training set, and the resulting classifier was used
to predict labels of the test set.  The results of these experiments
are seen below in the results section. We also put code written in this project on Github\cite{repo}.

\input{Result}


%\section{Analysis}
\input{Analysis}


\section{Future Work}
Given extra time, we would like to improve our work in following directions.
\begin{enumerate}
	\item We would like to prune the dataset by merging ingredients which have different names while are essentially the same. 
	\item It would be good for us to analyze mislabeled data so that we may have useful observation to facilitate learning.
	\item Although we have used several machine learning algorithms in this project, we would like to try more parameter combinations of AdaBoost algorithm.
\end{enumerate}

\section{Conclusion}
Although we saw that multi-class classification can be difficult and
may lead to lower prediction accuracy, I think that we obtained a
reasonably high rate of around 80\%.  

We believe there may be an limit to the accuracy that is possible with
even the best algorithm, due to the reasons described in the beginning of the analysis section. And we proposed a method to ease the limit in the future work section.
%the fact that different cuisines can
%be indicated by the same set of ingredients.

%We saw that ensemble methods can definitely help to boost performance
%of single algorithms.

Overall we were pleased with the results of our implementation.  It
was a good experience, and it gave us a great opportunity to try
various learning algorithms on real world data.

\bibliography{refs}
\bibliographystyle{plain}
\end{document}