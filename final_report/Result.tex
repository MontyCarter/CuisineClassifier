\section{Results}
We ran machine learning algorithms on Emulab d820\cite{emulab-wiki} machines with four 2.2 GHz 64-bit 8-Core Intel E5-4620 processors and 128GB DDR3 memory. We utilized all the CPUs when doing multi-processing cross validation.

Experimental results are shown in the following table. Machine learning algorithms are grouped in the table and groups are separated with an extra line. We want to point out the different between SVC with linear kernel and Linear SVC: the former does one vs one classification while the latter does one vs the rest classification. Accuracy represents the accuracy of predicting test set with the best hyper parameter obtained from cross validation. Time is the time taken to train the classifier on the full training dataset and to predict labels of test dataset. We give averaged time instead of total time because the average time is approximate to the runtime for the whole training set. We only show part of the best hyper parameters which is different to the default settings of each machine learning algorithm. Moreover, NA in the optimal hyper parameter column for a machine learning algorithm indicates that there are few hyper parameters that we can choose or we believe cross validation would not improve accuracy significantly for that particular algorithm.

\begin{center}
	\captionof{table}{Experimental results}
	\begin{tabular}{c|c|c|c}
    \hline
	     Algorithm   & Accuracy & Time & Optimal hyper parameter\\
         \hline
         SVM SVC (rbf kernel)   & 78.20  & 839.8 & C: 5.0, gamma: 0.1\\
         \hline
         SVM SVC (poly kernel)  & 77.35  & 430.1 & C: 1000.0, coef0: 1.0, degree: 2\\
         \hline
         SVM SVC (linear kernel)   &  76.13  & 223.0 & C: 1.0\\
         \hline
         SVM LinearSVC    &  77.21   &362.9  & C: 1.0\\
         \hline
         \hline
         Gaussian Naive Bayes & 35.23 & 0.23 & NA \\
         \hline
         Bernoulli Naive Bayes &   70.71    & 0.28  & NA \\
         \hline
         Multinomial Naive Bayes & 73.62 	& 0.23  & NA \\
         \hline
         \hline
         Decision Tree &  61.23 & 10.1 & NA \\
         \hline
         \hline
         K nearest neighbors & 54.11 & 11.26 & k: 15, weights:  distance\\
         \hline
         \hline
         Random Forest & 72.10 & 156.59 & n\_estimators: 100\\
         \hline
         Extremely Randomized Trees & 73.44 & 356.2 &
         min\_samples\_split: 4,
         n\_estimators: 200 \\
         \hline
         AdaBoost (decision tree) & 55.66 & 126.4 & max\_depth: 1, learning\_rate : 1.0, n\_estimators: 100\\
         \hline
         AdaBoost (SVC) & 19.71 & 12057.7 & learning\_rate: 1.0,
         n\_estimators: 10\\
         \hline
          \end{tabular}
\end{center}