\section{Results}

Experimental results are shown in the following table. Machine
learning algorithms are grouped in the table and groups are separated
with an extra line. We want to point out the different between SVC
with linear kernel and Linear SVC: the former does one vs one
classification while the latter does one vs the rest classification. 

In the table below, the accuracy column represents the accuracy of
predicting labels for the test set with the best hyper parameter
obtained from cross validation. The time column includes the time
required for training the classifier on the full training set in
addition to the time required to predict the labels of the test
set. The hyper parameter column lists the best hyper parameter
combination found during cross-validation. 

\begin{table}
\begin{center}
	\captionof{table}{Experimental results}
	\begin{tabular}{c|c|c|c}
    \hline
	     Algorithm   & Accuracy & Time & Optimal hyper parameter\\
         \hline
         \hline
         SVM SVC (RBF Kernel)   & 78.20  & 839.8 & C: 5.0, gamma: 0.1\\
         \hline
         SVM SVC (Poly Kernel)  & 77.35  & 430.1 & C: 1000.0, coef0: 1.0, degree: 2\\
         \hline
         SVM SVC (Linear Kernel)   &  76.13  & 223.0 & C: 1.0\\
         \hline
         SVM LinearSVC    &  77.21   &362.9  & C: 1.0\\
         \hline
         \hline
         Gaussian Naive Bayes & 35.23 & 0.23 & NA \\
         \hline
         Bernoulli Naive Bayes &   70.71    & 0.28  & NA \\
         \hline
         Multinomial Naive Bayes & 73.62 	& 0.23  & NA \\
         \hline
         \hline
         Decision Tree &  61.23 & 10.1 & NA \\
         \hline
         \hline
         K-Nearest Neighbors & 54.11 & 11.26 & k: 15, weights:  distance\\
         \hline
         \hline
         Random Forest & 72.10 & 156.6 & n\_estimators: 100\\
         \hline
         %Extremely Randomized Trees & 73.44 & 356.2 &
         %min\_samples\_split: 4,
         %n\_estimators: 200 \\
         Extremely Randomized Trees & 73.44 & 356.2 &
         \shortstack{min\_samples\_split: 4\\
         n\_estimators: 200} \\
         \hline
         %AdaBoost (Decision Tree) & 55.66 & 126.4 & max\_depth: 1, learning\_rate : 1.0, n\_estimators: 100\\
         AdaBoost (Decision Tree) & 55.66 & 126.4 &\shortstack{ max\_depth: 1\\ learning\_rate: 1.0\\ n\_estimators: 100}\\
         \hline
         AdaBoost (SVC) & 19.71 & 12057.7 & learning\_rate: 1.0,
         n\_estimators: 10\\
         \hline
          \end{tabular}
\end{center}
\end{table}
